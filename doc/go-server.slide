Concurrent Server Architectures

Yasha Bubnov
Backend Engineer, Juno
ybubnov@gojuno.com

* Initial data

- Transfer Control Protocol.
- Clients with persisted connections.
- Multiple requests from a single connection.


* The C10K Problem

It's time for web servers to handle ten thousand clients simultaneously, don't you think?
Dan Kegel, 1999

.link http://www.kegel.com/c10k.html

- Serve many clients with each thread, and use nonblocking I/O.
- Serve many clients with each thread, and use asynchronous I/O.
- Build the server code into the kernel (khttpd).


* Sequential Server

- Create a server socket (listener).
- Accept a client connection.
- Handle requests.
- Close the connection.

.image img/1-pic.png 300 _

: Where is a link to Stevens book?


* Sequential Server

.code src/1-server.go /^func main/,/^}/

* Sequential Server

.image img/2-pic.png 300 _


* Concurrent Server

- Create a server socket (listener).
- Accept a client connection.
- Handle requests concurrently.
- Close the connection.

.image img/9-pic.png 300 _


* Forking

- Create a server socket.
- Start accepting client connections.
- Create a *process* to handle each new client.

.image img/3-pic.png 350 _


* Threading

- Create a server socket.
- Start accepting client connections.
- Create a *thread* to handle each new client.

.image img/4-pic.png 350 _


* Pre-forking & Pre-threading

- Create a server socket.
- Spawn N processes.
- Start accepting client connections.

.image img/5-pic.png 300 _
: Examples of real-word applications?


* Pre-forking & Pre-threading - Thundering Herd

When an incoming TCP connection is accepted, all waiting processes / threads are awaken.

.image img/6-pic.png 350 _


* Pre-forking - Thundering Herd

- Mutual exclusion through file locking.
- Passing file descriptors from parent process to children processes.

.image img/7-pic.png 300 _


* Pre-threading - Thundering Herd

- Mutual exclusion though shared semaphore.

.image img/8-pic.png 400 _
: Graphics?


* Pre-forking & Pre-threading in Go

If there is no solution to the problem then don't waste time worrying about it.

    func ForkExec(argv0 string, argv []string, attr *ProcAttr) (pid int, err error)
        Combination of fork and exec, careful to be thread safe.

.image img/10-pic.png 250 _


* User-Space Threading

- Scheduled by the language runtime library.
- Rely on the system threads.

.image img/11-pic.png 350 _


* Non-blocking I/O vs. Asynchronous I/O.

Correct questions:
- Non-blocking vs. Blocking.
- Asynchronous vs. Synchronous.

.image img/12-pic.png 300 _

: Benchmarks??
: Move this to the middle and explain each approach in details.


* Blocking Synchronous I/O

- Traditional `select`.
- Traditional `poll`.

.image img/13-pic.png 300 _


* Non-blocking Synchronous I/O

- Again `select`.
- Again `poll`.
- System call `epoll`.

.image img/14-pic.png 250 _


* Non-blocking Asynchronous I/O

- Family of `aio_*` system calls.

.image img/15-pic.png 250 _


* One Word About Parallelism in Go

.image img/16-pic.png 400 _


* Non-blocking Synchronous vs. Nonblocking Asynchronous I/O

.link https://benchmarksgame-team.pages.debian.net/benchmarksgame/faster/go-node.html

    source    secs    mem        cpu load
    Go        15.36   148,056    88%
    Node.js   62.94   1,858,960  70%


.link https://hackernoon.com/aws-lambda-go-vs-node-js-performance-benchmark-1c8898341982
    Max requests:        1000
    Concurrency level:   5
    Agent:               keepalive
    Requests per second: 10
                     Node.js    Go
    Mean latency:    252.2 ms   109.7 ms
     50%             203        91
     90%             384        151
     95%             478        197
     99%             894        435
    100%            8103       1133(longest request)


* Concurrent Server in Go

Serve each incoming connection in a standalone goroutine.
.code src/2-server.go /^func main/,/^}/


* Handle requests concurrently

.code src/2-server.go /^func serve/,/^}/

Handle each incoming request from a single connection in a standalone goroutine.
.code src/2-server.go /^func handle/,/^}/


* Trivial Client

.code src/3-client.go /^func main/,/^}/


* Testing

- One server with 256 MiB memory.
- One client connection with 10^6 requests.

Spawn a container with a limited amount of memory and no swap:

    docker run -m 256M --memory-swap 256M ...

Eventually OOM kills it:

    signal: killed


* Why?

- The minimum stack size in Go is 2 KiB:

.code src/4-stack.go /START OMIT/,/END OMIT/

- Math:

    2KiB * 10^6 ~ 2TB

* Worker Pool

.code src/5-server.go /START POOL OMIT/,/END POOL OMIT/


* Pre-goroutining Server

.code src/6-server.go /^func main/,/^}/


* Handling Requests

.code src/6-server.go /^func serve/,/^}/


* Painful Reality

- HTTP server from `"net/http"` does not _allow_ to setup handlers limit.
- HTTP2 server from `"x/net/http2"` does not _implement_ handlers limit.



* Conclusion

- Go uses non-blocking synchronous I/O multiplexing.
- Go is a _framework_ for writing high-performance servers.
